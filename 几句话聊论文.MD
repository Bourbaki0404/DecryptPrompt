很多论文之间重合度较高，精读其中一篇即可，更多论文只是帮助拓展下思路。
这里会把博客未覆盖的论文简单总结下, 不过这里的论文我也只是泛泛读了下，要是有总结不到位的地方，请指出~


## Chain-of-thought
#### 1. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models

zero-shot版本的Least-to-Most。用以下Prompt激活模型先分解问题再解决问题的能力：Let's first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan and solve the problem step by step

【类比：LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS】

#### 2. AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS 【AutoCOT】

自动化构建COT few-shot样本的方案。先论证了few-shot使用和问题相关且多样的样本效果更好。自动化方案如下
- 第一步使用SentenceBert对问题进行编码并聚类
- 第二步每个cluster中从类中心向外遍历Question并使用zero-shot-COT自动生成推理，如果推理满足筛选条件(步数<=5，token<=60)则使用该指令样本来代表cluster
之后直接使用以上生成的cluster样本来作为few-shot-cot

### 3. COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING

探索如何进一步提升COT的效果
- 结论1：更难(推理步骤更多)的few-shot样本效果更好，不好计算推理步数的样本可以用问题长度来替代。
- 结论2：推理时更多推理步数的思维链(感觉应该是没有跳过部分推理更合理些，因为有些时候模型会跳过中间推理步骤）和self-consistency联合使用效果更好

【关联：self-consistency】

#### 4.  STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning  

类似半监督的方案，利用模型自己生成COT，然后不断迭代进行COT微调，来优化模型COT的效果，具体流程
step1. few-shot-cot让模型对一个无标注数据集生成COT推理，只保留推理结果正确的样本，因为正确的样本COT的质量更高
step2. 利用第一步生成的cot样本对模型微调，然后用微调后的模型对数据集生成新的COT重复第一步
第一步第二步不断迭代，每次微调都是从头训练。同时针对模型无论如何都不能回答对的问题，可以通过加入Hint的方式让模型反推出正确COT。
我还好奇Hint的prompt模板长啥样子，结果多项选择问题作者直接在正确答案后面写了个CORRECT哈哈哈，在指令微调时会把Hint丢掉。
其实就是半监督+主动学习的思路

【关联：The CoT Collection等COT微调方案】

#### 5. LARGE LANGUAGE MODELS CAN SELF-IMPROVE

同STaR是COT微调方案，不过是使用Self-Consistency来生成COT样本。
同时和Specializing Smaller Language Models towards Multi-Step Reasoning相同论文采用了多种COT样本格式，包括few-shot-COT，
zero-shot-cot等等，来避免模型对样本格式过拟合。

【关联：SelfConsistency, StAR】

#### 6. Large Language Models Can Be Easily Distracted by Irrelevant Context


## LLM调用工具
#### 1. Augmented Large Language Models with Parametric Knowledge Guiding

使用大模型作为外挂知识库，先使用领域知识构建指令微调样本，再进行大模型微调，然后使用模型生成回答context，再基于Context让GPT回答。
哈哈但是在业界使用大家可不看回答准确率，使用外挂搜索和知识库的引用那是为了给用户检(甩)验(锅)真理的机会！
不过感觉这个思路可以替代一些传统槽位填充的方案

【关联：REPLUG】

#### 2. DecomP: Decomposed Prompting: A Modular Approach for Solving Complex Tasks 

和Least-to-Most问题分解的思路类似，Decomposer few-shot prompt把问题分解成几个子问题，提出了层次化和递归的问题分解方案。

【关联：Least-to-Most，Self-Ask】

#### 3.IRCOT:  Interleaving Retrieval with Chain-of-Thought Reasoning for knowledge Intensive Multi-Step Question

加入外部知识抽取,抽取+COT推理交替进行。近似于只有信息抽取动作的ReACT。适用于多步推理需要依赖上一步信息获取和回答的场景例如多跳QA。Retriver先抽取K个相关文档作为Context生成COT，每次只取第一步推理，并使用该推理作为query获取更多上下文加入已有上下文。
但是这种pipeline的方案在业界可行度不高，毕竟latency很难接受。

【关联：multi-step 开放域QA的一些其他方案例如SelfASk，ReAct，DecomP】


#### 4. ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models

把ReACT这类pipeline链式的调用方案，既后一步推理依赖前一步推理+工具调用+调用返回的方案，改成了利用LLM的推理能力一次性模拟生成所有推理和调用步骤[Planer]，并发调用所有工具[worker].
然后基于planer部分生成的类似于槽位填充的分析模板，把worker工具调用的返回结果填充进去，让LLM生成回答。
节省LLM token调用的同时大大优化整体推理的延时。

【关联：ReACT，SelfAsk】

#### 5. PAL: Program-aided Language Models

单一编程工具的调用方案。在COT中交替进行推理，和代码指令的编写。推理的部分以#开始，也就是转化成了代码注释的形势，用于之后的代码解码。举个栗子

```
Q: Roger has 5 tennis balls. He buys 2 more cans of
tennis balls. Each can has 3 tennis balls. How many
tennis balls does he have now?
A: Roger started with 5 tennis balls.
tennis_balls = 5
2 cans of 3 tennis balls each is
bought_balls = 2 * 3
tennis balls. The answer is
answer = tennis_balls + bought_balls
```


#### 6. Faithful Chain-of-Thought Reasoning

和PAL相似，不过在单一类型代码的基础上进行了拓展，支持更多的符号化表达。在思维链中加入符号化的执行命令。基于Prompt，让LM收到问题后，生成NL和SL交替出现的思维链，其中NL是问题拆解的自然语言语句和ReAct的Reason相同，SL则是任务相关的可执行工具调用命令。
和ReACT不同的是，工具的执行和生成工具调用拆分开了，在生成完整的思维链之后，才会统一执行所有的SL语句。在Math World Problem，MultiHop QA，PDDDL和Logical Inference上进行了测试。举个栗子

```
Q：If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the  parking lot?
# 1. How many cars are there in the beginning? (independent, support: ["there are 3 cars in the parking lot"])
n_cars_begin = 3
# 2. How many cars arrive? (independent, support: ["2 more cars arrive"])
n_cars_arrive = 2
# 3. Final answer: How many cars are in the parking lot? (depends on 1, 2)
n_cars_total = n_cars_begin + n_cars_arrive
```

【关联：ReAct，PAL 】

#### 7. REPLUG: Retrieval-Augmented Black-Box Language Models

固定大模型优化检索器的方案，之前的检索器优化方向主要是相关性，既更准确的计算query和检索Document之间的相关性。论文提出针对大模型的解码，可以有
一个新的优化目标。就是检索到的文档，在输入大模型作为context后，应该使得模型生成文本序列的perplexity更低，要达成这个目标损失函数的构造如下
1. 计算相似度分布：抽取TopK相关的文档，对K个文档相似度进行归一化 
2. 计算LM似然函数分布：每个文档作为context和query拼接，计算正确输出y的条件生成概率，把K个文档的条件生成概率进行归一化
3. 计算损失函数: 检索器的训练目标是最小化相似度分布和LM似然函数分布的KL散度。既保证相似度更高的Document，能提升模型正确token的解码置信度。


#### 8. TALM: Tool Augmented Language Models

谷歌2022年发表的语言模型工具使用的论文，算是ToolFormer的前辈。整体思路和toolformer很像，不过那时的模型能力跟不上。
一个可以借鉴的点是Self-play的样本生成方式，先标注少量的工具使用样本，微调模型，然后采样query，调用工具得到结果和模型输出，如果输出和标注相似，则保留样本。
然后基于补充的原本再微调模型，重复以上过程，使用了弱监督的训练方案。

【关联：ToolFormer】

#### 9. Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks

Self Ask的复杂优化版本，还是思维链推理和搜索工具结合。主要目的是提高LLM在正确的位置，正确使用外部检索，避免错误的Information Retrival破坏思维链。

1. 先用LLM直接生成全局推理的思维链（Chain-of-Query），其中每个节点都是一个QA对，推理对应的prompt如下

```
Construct a global reasoning chain for this complex [Question] : Where do greyhound buses that are in the birthplace of Spirit If...'s performer leave
from? You should generate a query to the search engine based on what you already know at each step of the reasoning chain, starting with [Query].
If you know the answer for [Query], generate it starting with [Answer].
You can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final Content].
If you don't know the answer, generate a query to search engine based on what you already know and do not know, starting with [Unsolved Query].
For example:
[Question]: Where do greyhound buses that are in the birthplace of Spirit If...'s performer leave from?
[Query 1]: Who is the performer of Spirit If... ?
If you don't know the answer:
[Unsolved Query]: Who is the performer of Spirit If... ?
If you know the answer:
[Answer 1]: The performer of Spirit If… is Kevin Drew
```

2. 然后遍历每个节点，引入IR搜索，针对LLM已经进行回答的节点的Q召回Top1的Document,然后引入一个在Open-Domain-QA上训练的MRC模型，基于Query和Document进行答案抽取，得到答案G。然后基于以下Prompt，使用LLM来对该节点的答案进行反思和校准。

```According to the Reference, the answer for qi should be g, you can change your answer and
continue constructing the reasoning chain for [Question]: Q. Reference: di
```

3. 针对LLM没有回答也就是以上的“Unsolved Query”，同样是用MRC进行答案抽取，然后用以下prompt，让LLM把答案填充到对应位置

```
According to the Reference, the answer for qi, should be g∗, you can give your answer and continue constructing the reasoning chain for [Question]:
Q. Reference: ∗i .
```
【关联：React， SelfAsk, 】

## 长文本

#### 1. Lost in the Middle: How Language Models Use Long Contexts

对GPT-3.5，Claude等闭源模型和LongChat-13B，MPT-30B等开源支持长文本的模型，在多文档（10~30个）问答和抽取上进行了长文本能力评估。得到以下几点发现
- 模型对于首尾文档的理解效果更好，等相关信息出现在开头和结尾的时候效果显著更好，中间部分效果存在20%+的效果下降，效果存在U型曲线。所以多文档作为上文可以通过rerank把重要的放在首尾
- 即便是长文本增强的模型，当上文长度上升(10个文档->30个文档)模型效果也会有显著下降，GPT-3.5-turbo，GPT-3.5-turbo(16K)在超长文档上效果没有很显著的差异



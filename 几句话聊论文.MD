很多论文之间重合度较高，精读其中一篇即可，更多论文只是帮助拓展下思路。
这里会把博客未覆盖的论文简单总结下~


## Chain-of-thought
1. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models
zero-shot版本的Least-to-Most。用以下Prompt激活模型先分解问题再解决问题的能力：Let's first understand the problem and devise a plan to solve the problem. Then, let’s carry out the plan and solve the problem step by step
【类比：LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS】

2. AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS 【AutoCOT】
自动化构建COT few-shot样本的方案。先论证了few-shot使用和问题相关且多样的样本效果更好。自动化方案如下
第一步使用SentenceBert对问题进行编码并聚类
第二步每个cluster中从类中心向外遍历Question并使用zero-shot-COT自动生成推理，如果推理满足筛选条件(步数<=5，token<=60)则使用该指令样本来代表cluster
之后直接使用以上生成的cluster样本来作为few-shot-cot


## LLM调用工具
1. Augmented Large Language Models with Parametric Knowledge Guiding
使用大模型作为外挂知识库，先使用领域知识构建指令微调样本，再进行大模型微调，然后使用模型生成回答context，再基于Context让GPT回答。
哈哈但是在业界使用大家可不看回答准确率，使用外挂搜索和知识库的引用那是为了给用户检(甩)验(锅)真理的机会！
不过感觉这个思路可以替代一些传统槽位填充的方案
【类比：REPLUG: Retrieval-Augmented Black-Box Language Models】

2. Decomposed Prompting: A Modular Approach for Solving Complex Tasks 【DecomP】
和Least-to-Most问题分解的思路类似，Decomposer few-shot prompt把问题分解成几个子问题，提出了层次化和递归的问题分解方案。
【类比：Least-to-Most，Self-Ask】

3. Interleaving Retrieval with Chain-of-Thought Reasoning for knowledge Intensive Multi-Step Question【IRCOT】
加入外部知识抽取,抽取+COT推理交替进行。近似于只有信息抽取动作的ReACT。适用于多步推理需要依赖上一步信息获取和回答的场景例如多跳QA。Retriver先抽取K个相关文档作为Context生成COT，每次只取第一步推理，并使用该推理作为query获取更多上下文加入已有上下文。
但是这种pipeline的方案在业界可行度不高，毕竟latency很难接受。
【类比：multi-step 开放域QA的一些其他方案例如SelfASk，ReAct，DecomP】

